# Abstractive-Text-Summarizer

Neural sequence-to-sequence models have provided a viable innovative approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we have used standard Long Short-Term Memory(LSTM) sequence-to-sequence attentional model. This method utilizes a local attention model for generating each word of the summary conditioned on the input sentence. While the model is structurally simple, it can easily be trained end-to-end and scales to a large amount of training data. We apply our model to the Amazon-fine-food-review dataset. We evaluate the reconstructed paragraph using standard metrics like ROUGE, showing that neural models can encode texts in a way that preserve syntactic, semantic, and discourse coherence.
